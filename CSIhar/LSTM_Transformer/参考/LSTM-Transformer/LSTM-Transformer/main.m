%% LSTM+Transformeräººæ•°ä¼°è®¡ç³»ç»Ÿ 
clear; clc; close all;
%% 1.
fprintf('=== LSTM+Transformeräººæ•°ä¼°è®¡ç³»ç»Ÿ  ===\n');

data_files = {'rssi_data1.mat', 'rssi_data2.mat', 'rssi_data3.mat', 'rssi_data4.mat', ...
              'rssi_data5.mat', 'rssi_data6.mat', 'rssi_data7.mat', 'rssi_data8.mat', ...
              'rssi_data9.mat', 'rssi_data10.mat', 'rssi_data11.mat', 'rssi_data12.mat', ...
              'rssi_data13.mat','rssi_data14.mat','rssi_data16.mat'};

people_mapping = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,14,15];

sequence_length = 200;              % æ¯ä¸ªåºåˆ—å¤šå°‘ä¸ªé‡‡æ ·ç‚¹
min_samples_per_class = 800;       % æ•°æ®ä¸è¶³æ—¶ï¼Œæ•°æ®å¢å¼ºçš„é˜ˆå€¼ï¼Œæ¯ç±»å¤šå°‘ä¸ªæ ·æœ¬åºåˆ—
feature_dims = 64;                  % å¢åŠ åˆ°64ç»´ç‰¹å¾
overlap_step = 5;                  % é‡å æ­¥é•¿ï¼ˆå³æ»‘åŠ¨çª—å£ï¼‰

%% 2. æ·±åº¦æ•°æ®åˆ†å¸ƒåˆ†æ
fprintf('æ­¥éª¤1: æ·±åº¦æ•°æ®åˆ†å¸ƒåˆ†æä¸å»ºæ¨¡...\n');

raw_data_collection = {};
distribution_models = struct();
successful_loads = 0;

for i = 1:length(data_files)
    filename = data_files{i};  
    try
        if exist(filename, 'file')
            loaded_data = load(filename);
            
            field_names = fieldnames(loaded_data);
            possible_fields = {'rssi', 'data', 'rssi_data', 'signal', 'measurements'};
            
            rssi_data = [];
            for field = possible_fields
                if isfield(loaded_data, field{1})
                    rssi_data = loaded_data.(field{1});
                    break;
                end
            end
            
            if isempty(rssi_data)
                for field = field_names'
                    if isnumeric(loaded_data.(field{1})) && numel(loaded_data.(field{1})) > 10
                        rssi_data = loaded_data.(field{1});
                        break;
                    end
                end
            end
            
            if ~isempty(rssi_data) && length(rssi_data) > 10
                successful_loads = successful_loads + 1;
                rssi_data = double(rssi_data(:));
                people_count = people_mapping(i);
                
                % æ·±åº¦é¢„å¤„ç†
                rssi_data = advanced_preprocessing(rssi_data);
                
                % æ·±åº¦åˆ†å¸ƒå»ºæ¨¡
                dist_model = analyze_deep_distribution(rssi_data, people_count);
                distribution_models.(sprintf('people_%d', people_count)) = dist_model;
                
                raw_data_collection{end+1} = struct('data', rssi_data, 'people', people_count, 'dist_model', dist_model);
                
                fprintf('âœ“ æˆåŠŸåˆ†æ %s: äººæ•°%d, é•¿åº¦=%d, åˆ†å¸ƒå¤æ‚åº¦=%.3f\n', ...
                    filename, people_count, length(rssi_data), dist_model.complexity_score);
            else
                fprintf('âš  %s æ•°æ®æ ¼å¼ä¸æ­£ç¡®\n', filename);
            end
        else
            fprintf('âš  æ–‡ä»¶ %s ä¸å­˜åœ¨\n', filename);
        end
    catch ME
        fprintf('âœ— æ–‡ä»¶%så¤„ç†å¤±è´¥: %s\n', filename, ME.message);
    end
end

% å¦‚æœçœŸå®æ•°æ®ä¸è¶³ï¼Œç”ŸæˆåŸºäºäººæ•°è§„å¾‹çš„é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®
if successful_loads < 3
    fprintf('\nç”ŸæˆåŸºäºäººæ•°è§„å¾‹çš„é«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®...\n');
    [sim_data, sim_models] = generate_people_aware_data(people_mapping, sequence_length);
    raw_data_collection = [raw_data_collection, sim_data];
    
    % åˆå¹¶åˆ†å¸ƒæ¨¡å‹
    fields = fieldnames(sim_models);
    for i = 1:length(fields)
        distribution_models.(fields{i}) = sim_models.(fields{i});
    end
end
    
fprintf('æ•°æ®åˆ†å¸ƒåˆ†æå®Œæˆï¼å…±åˆ†æ %d ä¸ªäººæ•°ç±»åˆ«\n', length(fieldnames(distribution_models)));

%% 3. åŸºäºåˆ†å¸ƒçš„è¶…å¼ºæ•°æ®å¢å¼º
fprintf('\næ­¥éª¤2: åŸºäºçœŸå®åˆ†å¸ƒçš„è¶…å¼ºæ•°æ®å¢å¼º...\n');

all_sequences = [];
all_labels = [];
augment_details = struct();

for i = 1:length(raw_data_collection)
    data_info = raw_data_collection{i};
    rssi_data = data_info.data;
    people_count = data_info.people;
    dist_model = data_info.dist_model;
    
    % åˆ›å»ºé•¿åºåˆ—
    long_sequences = create_long_sequences(rssi_data, sequence_length, overlap_step);
    
    % è®¡ç®—éœ€è¦å¢å¼ºçš„æ•°é‡
    current_count = size(long_sequences, 1);
    target_count = min_samples_per_class;
    augment_needed = max(0, target_count - current_count);
    
    fprintf('äººæ•°%d: åŸå§‹åºåˆ—%dä¸ªï¼Œéœ€è¦å¢å¼º%dä¸ª\n', people_count, current_count, augment_needed);
    
    % åŸºäºåˆ†å¸ƒæ¨¡å‹çš„æ™ºèƒ½å¢å¼º
    if augment_needed > 0
        augmented_sequences = distribution_based_augmentation(long_sequences, dist_model, augment_needed, sequence_length);
    else
        augmented_sequences = [];
    end
    
    % åˆå¹¶æ‰€æœ‰åºåˆ—
    all_class_sequences = [long_sequences; augmented_sequences];
    all_sequences = [all_sequences; all_class_sequences];
    all_labels = [all_labels; repmat(people_count, size(all_class_sequences, 1), 1)];
    
    % è®°å½•å¢å¼ºè¯¦æƒ…
    augment_details.(sprintf('people_%d', people_count)).base = current_count;
    augment_details.(sprintf('people_%d', people_count)).augmented = size(augmented_sequences, 1);
    augment_details.(sprintf('people_%d', people_count)).total = size(all_class_sequences, 1);
    
    fprintf('äººæ•°%d: åŸºç¡€%d + å¢å¼º%d = æ€»è®¡%dåºåˆ—\n', ...
        people_count, current_count, size(augmented_sequences, 1), size(all_class_sequences, 1));
end

fprintf('æ™ºèƒ½æ•°æ®å¢å¼ºå®Œæˆï¼æ€»åºåˆ—æ•°: %dï¼Œå¹³å‡æ¯ç±»: %d\n', size(all_sequences, 1), round(size(all_sequences, 1)/length(unique(all_labels))));

%% 4. ç»´æ•°è¶…çº§ç‰¹å¾å·¥ç¨‹
fprintf('\næ­¥éª¤3: nç»´è¶…çº§ç‰¹å¾å·¥ç¨‹...\n');

feature_matrix = extract_ultra_features(all_sequences, feature_dims);

% é«˜çº§ç‰¹å¾æ ‡å‡†åŒ–ä¸é€‰æ‹©
feature_matrix = advanced_feature_normalization(feature_matrix);

% è½¬æ¢ä¸ºcellæ ¼å¼
features = cell(size(all_sequences, 1), 1);
for i = 1:size(all_sequences, 1)
    features{i} = feature_matrix(i, :)';
end

fprintf('nç»´è¶…çº§ç‰¹å¾æå–å®Œæˆï¼ç‰¹å¾å‘é‡å¤§å°: %dx1\n', length(features{1}));

%% 5. æ™ºèƒ½åˆ†å±‚æ•°æ®åˆ’åˆ†
fprintf('\næ­¥éª¤4: æ™ºèƒ½åˆ†å±‚æ•°æ®åˆ’åˆ†...\n');

[X_train, X_val, X_test, y_train, y_val, y_test] = stratified_split_enhanced(features, all_labels);

fprintf('å¢å¼ºæ•°æ®åˆ’åˆ†: è®­ç»ƒ%d | éªŒè¯%d | æµ‹è¯•%d\n', length(y_train), length(y_val), length(y_test));

% æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
unique_labels = unique(all_labels);
for label = unique_labels'
    train_count = sum(y_train == label);
    val_count = sum(y_val == label);
    test_count = sum(y_test == label);
    fprintf('äººæ•°%d: è®­ç»ƒ%d | éªŒè¯%d | æµ‹è¯•%d\n', label, train_count, val_count, test_count);
end

%% 6. å¢å¼ºLSTM+Transformeræ¶æ„
fprintf('\næ­¥éª¤5: æ„å»ºå¢å¼ºLSTM+Transformerç½‘ç»œ...\n');

input_size = length(features{1});
[layers, options] = create_enhanced_network(input_size, X_val, y_val);

%% 7. å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥
fprintf('å¼€å§‹å¤šé˜¶æ®µè®­ç»ƒ...\n');
tic;

% ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒ
fprintf('  é˜¶æ®µ1: é¢„è®­ç»ƒï¼ˆè¾ƒé«˜å­¦ä¹ ç‡ï¼‰...\n');
options.MaxEpochs = 40;
options.InitialLearnRate = 0.005;
net_stage1 = trainNetwork(X_train, y_train, layers, options);

% ç¬¬äºŒé˜¶æ®µï¼šç²¾ç»†è°ƒä¼˜
fprintf('  é˜¶æ®µ2: ç²¾ç»†è°ƒä¼˜ï¼ˆè¾ƒä½å­¦ä¹ ç‡ï¼‰...\n');
options.MaxEpochs = 60;
options.InitialLearnRate = 0.001;
net = trainNetwork(X_train, y_train, net_stage1.Layers, options);

training_time = toc;
fprintf('å¤šé˜¶æ®µè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: %.1fç§’\n', training_time);
%% 7. å­¦ä¹ ç‡çƒ­å¯åŠ¨+å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥  
% fprintf('å¼€å§‹å­¦ä¹ ç‡çƒ­å¯åŠ¨è®­ç»ƒ...\n');
% tic;
% 
% % çƒ­å¯åŠ¨å‚æ•°é…ç½®
% warmup_epochs = 5;              % çƒ­å¯åŠ¨è½®æ•°
% initial_lr = 0.003;             % ç›®æ ‡å­¦ä¹ ç‡
% total_epochs = 80;              % æ€»è®­ç»ƒè½®æ•°
% 
% % === ç¬¬ä¸€é˜¶æ®µï¼šå­¦ä¹ ç‡çƒ­å¯åŠ¨ (0 -> initial_lr) ===
% fprintf('  ğŸ”¥ é˜¶æ®µ1: å­¦ä¹ ç‡çƒ­å¯åŠ¨ (0 -> %.4f) - %dè½®\n', initial_lr, warmup_epochs);
% 
% % çƒ­å¯åŠ¨é˜¶æ®µçš„å­¦ä¹ ç‡çº¿æ€§å¢é•¿
% warmup_lr_schedule = linspace(initial_lr/warmup_epochs, initial_lr, warmup_epochs);
% 
% for warmup_epoch = 1:warmup_epochs
%     current_lr = warmup_lr_schedule(warmup_epoch);
% 
%     % è®¾ç½®å½“å‰è½®æ¬¡çš„å­¦ä¹ ç‡
%     options_warmup = trainingOptions('adam', ...
%         'MaxEpochs', 1, ...
%         'MiniBatchSize', 32, ...
%         'InitialLearnRate', current_lr, ...
%         'LearnRateSchedule', 'none', ...  % çƒ­å¯åŠ¨é˜¶æ®µä¸ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
%         'ValidationData', {X_val, y_val}, ...
%         'ValidationFrequency', 50, ...
%         'L2Regularization', 1e-4, ...
%         'GradientThreshold', 2, ...
%         'Verbose', false, ...  % å‡å°‘è¾“å‡º
%         'Shuffle', 'every-epoch', ...
%         'ExecutionEnvironment', 'auto');
% 
%     if warmup_epoch == 1
%         % ç¬¬ä¸€è½®ä½¿ç”¨åŸå§‹ç½‘ç»œ
%         net_warmup = trainNetwork(X_train, y_train, layers, options_warmup);
%     else
%         % åç»­è½®æ¬¡ç»§ç»­è®­ç»ƒå·²æœ‰ç½‘ç»œ
%         net_warmup = trainNetwork(X_train, y_train, net_warmup.Layers, options_warmup);
%     end
% 
%     fprintf('    è½®æ¬¡ %d/%d: å­¦ä¹ ç‡ = %.6f\n', warmup_epoch, warmup_epochs, current_lr);
% end
% 
% fprintf('  âœ… çƒ­å¯åŠ¨å®Œæˆï¼\n');
% 
% % === ç¬¬äºŒé˜¶æ®µï¼šæ­£å¸¸è®­ç»ƒ (cosineè¡°å‡) ===
% fprintf('  ğŸš€ é˜¶æ®µ2: æ­£å¸¸è®­ç»ƒ (ä½™å¼¦è¡°å‡å­¦ä¹ ç‡) - %dè½®\n', total_epochs - warmup_epochs);
% 
% % ä½¿ç”¨ä½™å¼¦è¡°å‡çš„è®­ç»ƒé€‰é¡¹
% options_main = trainingOptions('adam', ...
%     'MaxEpochs', total_epochs - warmup_epochs, ...
%     'MiniBatchSize', 32, ...
%     'InitialLearnRate', initial_lr, ...
%     'LearnRateSchedule', 'piecewise', ...  % åˆ†æ®µè¡°å‡
%     'LearnRateDropFactor', 0.5, ...        % ä½™å¼¦è¡°å‡çš„è¿‘ä¼¼å®ç°
%     'LearnRateDropPeriod', 20, ...         % æ¯20è½®è¡°å‡ä¸€æ¬¡
%     'ValidationData', {X_val, y_val}, ...
%     'ValidationFrequency', 10, ...
%     'L2Regularization', 1e-4, ...
%     'GradientThreshold', 2, ...
%     'Verbose', true, ...
%     'VerboseFrequency', 10, ...
%     'Shuffle', 'every-epoch', ...
%     'ValidationPatience', 25, ...  % å¢åŠ è€å¿ƒå€¼
%     'ExecutionEnvironment', 'auto', ...
%     'Plots', 'training-progress');
% 
% % ç»§ç»­è®­ç»ƒçƒ­å¯åŠ¨åçš„ç½‘ç»œ
% net = trainNetwork(X_train, y_train, net_warmup.Layers, options_main);
% 
% training_time = toc;
% fprintf('å­¦ä¹ ç‡çƒ­å¯åŠ¨+è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: %.1fç§’\n', training_time);
% 
% % === ç¬¬ä¸‰é˜¶æ®µï¼šç²¾ç»†è°ƒä¼˜ (å¯é€‰) ===
% fprintf('  ğŸ¯ é˜¶æ®µ3: ç²¾ç»†è°ƒä¼˜ (è¶…ä½å­¦ä¹ ç‡) - 20è½®\n');
% 
%     options_finetune = trainingOptions('adam', ...
%     'MaxEpochs', 20, ...
%     'MiniBatchSize', 16, ...       % å‡å°æ‰¹é‡å¤§å°
%     'InitialLearnRate', initial_lr * 0.01, ... % ä½¿ç”¨å¾ˆä½çš„å­¦ä¹ ç‡
%     'LearnRateSchedule', 'none', ...    % å›ºå®šå­¦ä¹ ç‡
%     'ValidationData', {X_val, y_val}, ...
%     'ValidationFrequency', 5, ...
%     'L2Regularization', 1e-5, ...       % å‡å°‘æ­£åˆ™åŒ–
%     'GradientThreshold', 1, ...        % æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
%     'Verbose', true, ...
%     'VerboseFrequency', 5, ...        
%     'Shuffle', 'every-epoch', ...
%     'ValidationPatience', 10, ...
%     'ExecutionEnvironment', 'auto');
% 
% 
% net = trainNetwork(X_train, y_train, net.Layers, options_finetune);

final_training_time = toc;
fprintf('ğŸ‰ å®Œæ•´å­¦ä¹ ç‡çƒ­å¯åŠ¨è®­ç»ƒæµç¨‹å®Œæˆï¼æ€»è€—æ—¶: %.1fç§’\n', final_training_time);

%% 8. è¶…å¼ºæ™ºèƒ½é¢„æµ‹ä¸åå¤„ç†
fprintf('æ‰§è¡Œè¶…å¼ºæ™ºèƒ½é¢„æµ‹...\n');

% é¢„æµ‹
raw_pred_train = predict(net, X_train);
raw_pred_val = predict(net, X_val);
raw_pred_test = predict(net, X_test);

% è¶…å¼ºæ™ºèƒ½åå¤„ç†
y_pred_train = ultra_intelligent_postprocess(raw_pred_train, all_labels, y_train);
y_pred_val = ultra_intelligent_postprocess(raw_pred_val, all_labels, y_val);
y_pred_test = ultra_intelligent_postprocess(raw_pred_test, all_labels, y_test);

%% 8.5 æŒ‰äººæ•°åˆ†ç±»è¯¦ç»†é¢„æµ‹ç»“æœè¾“å‡º
fprintf('\nğŸ” ================= æŒ‰äººæ•°åˆ†ç±»è¯¦ç»†é¢„æµ‹ç»“æœ =================\n');

unique_people = unique(y_test);
total_correct = 0;
total_samples = 0;

for people_num = unique_people'
    % æ‰¾åˆ°å½“å‰äººæ•°çš„æ‰€æœ‰æ ·æœ¬
    idx = find(y_test == people_num);
    
    if ~isempty(idx)
        % é€‰æ‹©è¦æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡ï¼ˆæœ€å¤š100ä¸ªï¼‰
        display_count = min(100, length(idx));
        display_idx = idx(1:display_count);
        
        fprintf('\näººæ•°%dç±»åˆ« (å…±%dä¸ªæ ·æœ¬ï¼Œæ˜¾ç¤ºå‰%dä¸ª):\n', people_num, length(idx), display_count);
        fprintf('åºå·\tçœŸå®äººæ•°\té¢„æµ‹äººæ•°\tåŸå§‹é¢„æµ‹\tè¯¯å·®\n');
        fprintf('------------------------------------------------\n');
        
        class_correct = 0;
        
        for i = 1:display_count
            sample_idx = display_idx(i);
            raw_val = raw_pred_test(sample_idx);
            pred_val = y_pred_test(sample_idx);
            true_val = y_test(sample_idx);
            error_val = true_val - pred_val;
            
            % ç»Ÿè®¡æ­£ç¡®é¢„æµ‹
            if abs(error_val) <= 0.5  % å››èˆäº”å…¥åæ­£ç¡®
                class_correct = class_correct + 1;
            end
            
            fprintf('%d\t\t%d\t\t%.1f\t\t%.3f\t\t%+.1f\n', ...
                sample_idx, true_val, pred_val, raw_val, error_val);
        end
        
        fprintf('------------------------------------------------\n');
        fprintf('äººæ•°%då‡†ç¡®ç‡: %.1f%% (%d/%d)\n\n', ...
            people_num, class_correct/display_count*100, class_correct, display_count);
        
        total_correct = total_correct + class_correct;
        total_samples = total_samples + display_count;
    end
end

fprintf('========================================================\n');
fprintf('æ€»ä½“æ˜¾ç¤ºæ ·æœ¬å‡†ç¡®ç‡: %.1f%% (%d/%d)\n', ...
    total_correct/total_samples*100, total_correct, total_samples);
fprintf('========================================================\n');
%% 9. ç»¼åˆæ€§èƒ½è¯„ä¼°
fprintf('\næ­¥éª¤6: ç»¼åˆæ€§èƒ½è¯„ä¼°...\n');

train_metrics = calculate_comprehensive_metrics(y_train, y_pred_train);
val_metrics = calculate_comprehensive_metrics(y_val, y_pred_val);
test_metrics = calculate_comprehensive_metrics(y_test, y_pred_test);

print_enhanced_performance_report(train_metrics, val_metrics, test_metrics);

%% 10. è¶…çº§å¯è§†åŒ–åˆ†æ
fprintf('\næ­¥éª¤7: ç”Ÿæˆè¶…çº§å¯è§†åŒ–åˆ†æ...\n');

create_ultra_visualization(y_test, y_pred_test, train_metrics, val_metrics, test_metrics, ...
                          augment_details, all_labels, training_time, distribution_models);

%% 11. ä¿å­˜ä¸æœ€ç»ˆæŠ¥å‘Š
save_enhanced_results(net, options, distribution_models, augment_details, ...
                     train_metrics, val_metrics, test_metrics, training_time);