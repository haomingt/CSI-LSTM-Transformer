import torch
import torch.nn as nn
import torch.nn.functional as F
import sys

# é€‚é…æœåŠ¡å™¨è·¯å¾„ï¼ˆä¸data_processor_BLSTM.pyä¿æŒä¸€è‡´ï¼‰
sys.path.append("/home/chenjun/python_codes/WiFi_sensing/scripts")


class AttenLayer(nn.Module):
    """
    å½»åº•ä¿®å¤ç»´åº¦é”™è¯¯ï¼šæ›´æ¢1ç»´å‚æ•°çš„åˆå§‹åŒ–æ–¹å¼ï¼Œé¿å…Xavierå¯¹1ç»´å¼ é‡çš„é™åˆ¶
    """

    def __init__(self, num_state=400):
        super(AttenLayer, self).__init__()
        self.num_state = num_state
        # é¢„å®šä¹‰å‚æ•°ï¼ˆæ˜ç¡®æ ‡æ³¨ç»´åº¦ï¼‰
        self.kernel = None  # 2ç»´ï¼š(hidden_dim, num_state)
        self.bias = None  # 1ç»´ï¼š(num_state,)
        self.prob_kernel = None  # 1ç»´ï¼š(num_state,)

    def forward(self, input_tensor):
        batch_size, seq_len, hidden_dim = input_tensor.shape

        # åˆå§‹åŒ–å‚æ•°ï¼šæŒ‰ç»´åº¦ç±»å‹é€‰æ‹©åˆé€‚çš„åˆå§‹åŒ–æ–¹å¼
        if self.kernel is None:
            # 1. 2ç»´æƒé‡çŸ©é˜µï¼ˆkernelï¼‰ï¼šç»§ç»­ç”¨Xavieråˆå§‹åŒ–ï¼ˆé€‚åˆçº¿æ€§æ˜ å°„ï¼‰
            self.kernel = nn.Parameter(
                torch.empty((hidden_dim, self.num_state), device=input_tensor.device)
            )
            nn.init.xavier_uniform_(self.kernel)  # ä»…å¯¹2ç»´å¼ é‡ä½¿ç”¨Xavier

            # 2. 1ç»´åç½®ï¼ˆbiasï¼‰ï¼šç”¨å¸¸æ•°åˆå§‹åŒ–ï¼ˆ0å€¼ï¼Œé¿å…åˆå§‹å¹²æ‰°ï¼‰
            self.bias = nn.Parameter(
                torch.zeros(self.num_state, device=input_tensor.device)
            )

            # 3. 1ç»´æ¦‚ç‡æƒé‡ï¼ˆprob_kernelï¼‰ï¼šç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼ˆæ›¿ä»£Xavierï¼Œé¿å…ç»´åº¦é”™è¯¯ï¼‰
            self.prob_kernel = nn.Parameter(
                torch.empty(self.num_state, device=input_tensor.device)
            )
            nn.init.uniform_(self.prob_kernel, a=-0.1, b=0.1)  # 1ç»´å¼ é‡é€‚é…çš„åˆå§‹åŒ–

        # æ³¨æ„åŠ›è®¡ç®—é€»è¾‘ï¼ˆåŠŸèƒ½ä¸å˜ï¼Œç¡®ä¿ä¸åŸç‰ˆæœ¬ä¸€è‡´ï¼‰
        atten_state = torch.tanh(torch.matmul(input_tensor, self.kernel) + self.bias)  # (batch, seq_len, num_state)
        logits = torch.matmul(atten_state, self.prob_kernel)  # (batch, seq_len)
        time_weights = F.softmax(logits, dim=1)  # æƒé‡å½’ä¸€åŒ–
        weighted_feature = torch.sum(input_tensor * time_weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim)

        return weighted_feature


def build_blstm_model(input_shape=(100, 52), num_classes=7):
    """
    æ„å»ºä¸æ•°æ®å¤„ç†è„šæœ¬å®Œå…¨å…¼å®¹çš„BLSTMæ¨¡å‹
    è¾“å…¥å½¢çŠ¶(100,52)åŒ¹é…data_processor_BLSTM.pyçš„seq_len=100ã€52å­è½½æ³¢
    """

    class BLSTMAttentionModel(nn.Module):
        def __init__(self, input_dim, hidden_dim=200, num_classes=7):
            super(BLSTMAttentionModel, self).__init__()
            # åŒå‘LSTMï¼šå•å±‚æ— å†…ç½®dropoutï¼Œæ¶ˆé™¤è­¦å‘Š
            self.blstm = nn.LSTM(
                input_size=input_dim,
                hidden_size=hidden_dim,
                bidirectional=True,
                batch_first=True,
                dropout=0.0,
                num_layers=1
            )
            # ç‹¬ç«‹Dropoutå±‚ï¼šä¿æŒæ­£åˆ™åŒ–æ•ˆæœ
            self.dropout = nn.Dropout(p=0.2)
            self.attention = AttenLayer(num_state=400)
            self.fc = nn.Linear(hidden_dim * 2, num_classes)  # åŒå‘è¾“å‡º400ç»´

        def forward(self, x):
            # å‰å‘ä¼ æ’­ï¼šä¸¥æ ¼åŒ¹é…æ•°æ®æ ¼å¼
            blstm_out, _ = self.blstm(x)  # (batch, 100, 400)
            blstm_out = self.dropout(blstm_out)  # æ­£åˆ™åŒ–
            atten_out = self.attention(blstm_out)  # (batch, 400)
            logits = self.fc(atten_out)  # (batch, 7)
            return F.softmax(logits, dim=1)  # è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ

    seq_len, input_dim = input_shape
    model = BLSTMAttentionModel(
        input_dim=input_dim,
        hidden_dim=200,
        num_classes=num_classes
    )

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print("=" * 60)
    print("=== æ³¨æ„åŠ›BLSTMæ¨¡å‹ï¼ˆPyTorchæœ€ç»ˆä¿®å¤ç‰ˆï¼‰ ===")
    print(f"è¾“å…¥è¦æ±‚: {input_shape} (æ—¶é—´æ­¥Ã—å­è½½æ³¢) | è¾“å‡º: (batch, 7) (æ´»åŠ¨æ¦‚ç‡)")
    print("\næ¨¡å‹ç»“æ„:")
    print(model)
    print("=" * 60)

    return model


if __name__ == "__main__":
    """æµ‹è¯•ï¼šç¡®ä¿æ— ç»´åº¦é”™è¯¯ï¼Œè¾“å…¥è¾“å‡ºå®Œå…¨åŒ¹é…"""
    try:
        # 1. æ„å»ºæ¨¡å‹ï¼ˆä¸æ•°æ®å¤„ç†è„šæœ¬å…¼å®¹ï¼‰
        model = build_blstm_model(input_shape=(100, 52))

        # 2. æ¨¡æ‹Ÿæ•°æ®ï¼šå®Œå…¨å¤åˆ»data_processor_BLSTM.pyçš„è¾“å‡ºæ ¼å¼
        # å½¢çŠ¶ï¼š(batch=32, seq_len=100, features=52)
        mock_input = torch.randn(32, 100, 52)

        # 3. å‰å‘ä¼ æ’­ï¼ˆå…³é—­æ¢¯åº¦ï¼Œä»…æµ‹è¯•æ¨ç†ï¼‰
        with torch.no_grad():
            mock_output = model(mock_input)

        # 4. éªŒè¯ç»“æœ
        print("\nâœ… æµ‹è¯•é€šè¿‡ï¼è¯¦ç»†ä¿¡æ¯:")
        print(f"  è¾“å…¥å½¢çŠ¶: {mock_input.shape} â†’ ç¬¦åˆé¢„æœŸ")
        print(f"  è¾“å‡ºå½¢çŠ¶: {mock_output.shape} â†’ ç¬¦åˆé¢„æœŸ (32,7)")
        print(f"  é¦–æ ·æœ¬æ¦‚ç‡å’Œ: {mock_output[0].sum().item():.4f} â†’ æ¥è¿‘1.0ï¼ˆsoftmaxæ­£å¸¸ï¼‰")
        print("\nâœ… æ¨¡å‹å¯ç›´æ¥ç”¨äºè®­ç»ƒï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼Œé”™è¯¯è¯¦æƒ…: {str(e)}")
        # é’ˆå¯¹æ€§æ’æŸ¥å»ºè®®
        if "Fan in and fan out" in str(e):
            print("ğŸ” é”™è¯¯åŸå› ï¼š1ç»´å¼ é‡ä½¿ç”¨äº†Xavieråˆå§‹åŒ–ï¼Œå·²åœ¨ä¿®å¤ç‰ˆä¸­è§£å†³ï¼Œè¯·é‡æ–°è¿è¡Œï¼")
        else:
            print("ğŸ” è¯·æ£€æŸ¥ï¼šPyTorchç‰ˆæœ¬ï¼ˆéœ€â‰¥1.8ï¼‰ã€è¾“å…¥å½¢çŠ¶æ˜¯å¦ä¸º(100,52)")